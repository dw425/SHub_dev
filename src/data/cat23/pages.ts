import { registerPages } from '../pageRegistry'
import type { PageData } from '../pageTypes'

const pages: PageData[] = [
  {
    slug: 'image-classification',
    badge: 'üè∑Ô∏è Page 23.1',
    title: 'Image Classification',
    description: 'Assign semantic labels to images using deep neural networks. From foundational CNNs to cutting-edge Vision Transformers, master the architectures, training strategies, optimization techniques, and deployment patterns that power production-grade classification systems.',
    accentColor: '#8B5CF6',
    accentLight: '#A78BFA',
    metrics: [
      { value: '91.1%', label: 'SOTA ImageNet Top-1' },
      { value: '22B', label: 'ViT-22B Parameters' },
      { value: '<1ms', label: 'Edge Inference' },
      { value: '14M+', label: 'ImageNet Images' },
    ],
    overview: {
      title: 'Image Classification',
      subtitle: '',
      subsections: [
        {
          heading: 'Classification Architectures',
          paragraphs: [
            'Neural network designs powering modern image classification',
            'Image classification architectures have evolved dramatically from hand-crafted features to deep convolutional networks and attention-based transformers, each generation bringing significant accuracy improvements. CNNs dominated the field from 2012 to 2020, with key innovations including residual connections (ResNet), dense connections (DenseNet), and efficient channel attention (SENet) enabling ever-deeper networks. Vision Transformers (ViT) emerged in 2020 from Google Research, demonstrating that pure attention mechanisms without any convolutions could match or exceed CNN performance when trained on sufficient data. The current frontier is hybrid architectures like ConvNeXt and CoAtNet that combine the inductive biases of convolutions (translation equivariance, local connectivity) with the global modeling capabilities of attention, achieving state-of-the-art results with improved training efficiency. Understanding these architectural trade-offs is essential for selecting the right model for your specific constraints around accuracy, latency, memory, and deployment target.',
          ],
        },
        {
          heading: 'Transfer Learning Strategies',
          paragraphs: [
            'Leverage pretrained models for domain-specific tasks',
            'Transfer learning is the most effective approach for real-world classification tasks, enabling teams to achieve production-quality results with a fraction of the data and compute required for training from scratch. Rather than randomly initializing weights, leverage models pretrained on large-scale datasets like ImageNet, which have already learned rich visual representations. Early convolutional layers capture universal low-level features such as edges, textures, and color gradients that transfer remarkably well across domains. Middle layers encode more complex patterns like shapes and object parts, while the final layers learn highly task-specific features that typically require fine-tuning. The key insight is that visual features follow a hierarchy from generic to specific, and this hierarchy can be exploited by freezing early layers while adapting later ones to your target domain.',
          ],
        },
        {
          heading: 'Data Augmentation',
          paragraphs: [
            'Essential techniques to improve generalization',
            'Data augmentation is one of the most powerful regularization techniques available, artificially expanding your training set by applying transformations that preserve semantic meaning. Basic geometric transforms like random flipping, rotation, and cropping teach models to recognize objects regardless of position or orientation in the frame. Color augmentations including brightness, contrast, and saturation jittering ensure models aren\'t overly reliant on specific color distributions that may vary in production. Advanced techniques like CutOut, MixUp, and CutMix have revolutionized augmentation by forcing models to learn more robust representations‚ÄîCutOut masks random patches, MixUp blends images and labels together, and CutMix combines regions from different images. RandAugment automates the search for optimal augmentation policies, applying randomly selected transforms with learned magnitudes that adapt to your specific dataset.',
          ],
        },
      ],
    },
    concepts: {
      title: 'Transfer Learning Strategies',
      subtitle: 'Core components and patterns',
      columns: 2,
      cards: [
        {
          className: 'strategy-0',
          borderColor: '#3B82F6',
          icon: '‚ùÑÔ∏è',
          title: 'Feature Extraction',
          description: 'Freeze all backbone layers. Train only the classification head. Fastest approach‚Äîworks with very small datasets (100-500 samples).',
          examples: [],
        },
        {
          className: 'strategy-1',
          borderColor: '#10B981',
          icon: 'üîß',
          title: 'Progressive Fine-tuning',
          description: 'Unfreeze top layers progressively. Use discriminative learning rates. Best balance of accuracy and efficiency for medium datasets.',
          examples: [],
        },
        {
          className: 'strategy-2',
          borderColor: '#8B5CF6',
          icon: 'üî•',
          title: 'Full Fine-tuning',
          description: 'Train all layers end-to-end with pretrained initialization. Requires more data but achieves highest accuracy for dissimilar domains.',
          examples: [],
        },
        {
          className: 'concept-3',
          borderColor: '#F59E0B',
          icon: 'üí°',
          title: 'Image Classification',
          description: 'Assign semantic labels to images using deep neural networks. From foundational CNNs to cutting-edge Vision Transformers, master the architectures, training strategies, optimization techniques, and dep',
          examples: [],
        },
      ],
    },
    hasSvgViz: true,
    algorithms: {
      type: 'card-grid',
      title: 'Model Comparison',
      subtitle: 'Evaluating approaches and tools',
      cards: [
        { icon: 'üõ†Ô∏è', title: 'ResNet-50', subtitle: '80.4%', description: 'Baseline', tags: ['80.4%'] },
        { icon: 'üõ†Ô∏è', title: 'EfficientNetV2-S', subtitle: '83.9%', description: 'Balanced', tags: ['83.9%'] },
        { icon: 'üõ†Ô∏è', title: 'ViT-Base/16', subtitle: '84.5%', description: 'Large Data', tags: ['84.5%'] },
        { icon: 'üõ†Ô∏è', title: 'Swin-Base', subtitle: '85.2%', description: 'Multi-task', tags: ['85.2%'] },
        { icon: 'üõ†Ô∏è', title: 'ConvNeXt-Base', subtitle: '85.8%', description: 'High Acc', tags: ['85.8%'] },
        { icon: 'üõ†Ô∏è', title: 'MobileNetV3-L', subtitle: '75.2%', description: 'Mobile', tags: ['75.2%'] },
      ],
    },
    tools: {
      title: 'Tools & Libraries',
      subtitle: 'Essential tools and platforms',
      items: [
        { icon: 'Py', name: 'PyTorch', vendor: '', description: 'Dynamic graphs, research-friendly', tags: [] },
        { icon: 'TF', name: 'TensorFlow', vendor: '', description: 'Production, TFLite, TPU', tags: [] },
        { icon: 'Ti', name: 'timm', vendor: '', description: '700+ pretrained models', tags: [] },
        { icon: 'ü§ó', name: 'Transformers', vendor: '', description: 'ViT, CLIP, easy fine-tune', tags: [] },
        { icon: 'TR', name: 'TensorRT', vendor: '', description: 'GPU optimization, INT8', tags: [] },
        { icon: 'ON', name: 'ONNX Runtime', vendor: '', description: 'Cross-platform deploy', tags: [] },
        { icon: 'Ab', name: 'Albumentations', vendor: '', description: '70+ fast transforms', tags: [] },
        { icon: 'WB', name: 'W&B', vendor: '', description: 'Experiment tracking', tags: [] },
        { icon: 'Rb', name: 'Roboflow', vendor: '', description: 'Dataset management', tags: [] },
        { icon: 'Gr', name: 'Gradio', vendor: '', description: 'Quick model demos', tags: [] },
        { icon: 'CM', name: 'CoreML', vendor: '', description: 'iOS/macOS deploy', tags: [] },
        { icon: 'LS', name: 'Label Studio', vendor: '', description: 'Open-source labeling', tags: [] },
      ],
    },
    bestPractices: {
      title: 'Best Practices',
      subtitle: 'Guidelines and recommendations',
      doItems: [
        'Always Start with Transfer Learning ‚Äî Fine-tuning pretrained weights converges 10-100x faster and generalizes better than random initialization.',
        'Use Progressive Resizing ‚Äî Train on 224px first, then increase to 384px+. Acts as regularization and speeds up initial training.',
        'Apply Strong Data Augmentation ‚Äî RandAugment, MixUp, CutMix dramatically improve generalization, especially on small datasets.',
        'Use Cosine LR Schedule ‚Äî Cosine annealing with warmup outperforms step decay. Include 5-10 warmup epochs for transformers.',
        'Handle Class Imbalance ‚Äî Use weighted loss, focal loss, or balanced sampling. Don\'t ignore‚Äîit causes silent failure on minority classes.',
        'Implement Early Stopping ‚Äî Monitor validation metrics with patience of 5-10 epochs. Save best checkpoint to prevent overfitting.',
        'Use Test-Time Augmentation ‚Äî Average predictions over flips and crops at inference. Adds 1-2% accuracy with minimal overhead.',
        'Calibrate Your Model ‚Äî Apply temperature scaling post-training. Neural networks are overconfident‚Äîverify with reliability diagrams.',
        'Use Stratified Splits ‚Äî Ensure train/val/test maintain class distributions. Use group-aware splitting if samples aren\'t independent.',
        'Monitor Multiple Metrics ‚Äî Track accuracy, per-class precision/recall, confusion matrix. Single metrics hide problems.',
      ],
      dontItems: [
      ],
    },
    agent: {
      avatar: 'ü§ñ',
      name: 'ClassificationEngineer',
      role: '',
      description: 'Expert agent for designing classification architectures, implementing transfer learning, handling class imbalance, optimizing training pipelines, and deploying calibrated models to production.',
      capabilities: [
        'Architecture selection & customization',
        'Transfer learning strategies',
        'Class imbalance handling',
        'Hyperparameter optimization',
        'Model calibration',
        'Production deployment',
      ],
      codeFilename: 'classification_pipeline.py',
      code: ``,
    },
    relatedPages: [
      { number: '23.2', title: 'Object Detection', description: 'YOLO, Faster R-CNN, DETR, real-time optimization', slug: 'object-detection' },
      { number: '23.3', title: 'Image Segmentation', description: 'Semantic, instance, panoptic with U-Net, SAM', slug: 'image-segmentation' },
      { number: '23.6', title: 'Generative Vision', description: 'Image generation, style transfer, diffusion', slug: 'generative-vision' },
    ],
    prevPage: undefined,
    nextPage: { title: '23.2 Object Detection', slug: 'object-detection' },
  },
  {
    slug: 'object-detection',
    badge: 'üéØ Page 23.2',
    title: 'Object Detection',
    description: 'Locate and classify multiple objects within images using bounding boxes. From real-time YOLO models to accurate two-stage detectors and transformer-based DETR, master the architectures, anchor strategies, and NMS techniques that power modern detection systems.',
    accentColor: '#10B981',
    accentLight: '#34D399',
    metrics: [
      { value: '63.4%', label: 'COCO mAP SOTA' },
      { value: '160 FPS', label: 'YOLOv8n Speed' },
      { value: '80', label: 'COCO Classes' },
      { value: '118K', label: 'Training Images' },
    ],
    overview: {
      title: 'Object Detection',
      subtitle: '',
      subsections: [
        {
          heading: 'Detection Pipeline',
          paragraphs: [
            'End-to-end flow from image to bounding box predictions',
            'Modern object detection pipelines follow a consistent pattern regardless of the specific architecture family. The input image first passes through a backbone network (typically a pretrained CNN or Vision Transformer) that extracts hierarchical features at multiple scales. A neck module such as Feature Pyramid Network (FPN) or Path Aggregation Network (PANet) then fuses these multi-scale features to handle objects of varying sizes. Finally, the detection head produces predictions‚Äîeither dense predictions across a grid (one-stage) or region proposals followed by refinement (two-stage)‚Äîand Non-Maximum Suppression (NMS) filters overlapping boxes to produce the final output.',
          ],
        },
        {
          heading: 'Detection Architectures',
          paragraphs: [
            'From real-time YOLO to accurate two-stage detectors',
            'Object detection architectures have evolved along two main branches: one-stage detectors optimized for speed and two-stage detectors prioritizing accuracy. The YOLO (You Only Look Once) family pioneered real-time detection by treating it as a single regression problem, predicting bounding boxes and class probabilities directly from full images in one evaluation. Two-stage detectors like Faster R-CNN first generate region proposals using a Region Proposal Network (RPN), then classify and refine each proposal, achieving higher accuracy at the cost of inference speed. More recently, DETR introduced transformer-based detection that eliminates hand-designed components like anchors and NMS by treating detection as a set prediction problem with bipartite matching. Understanding these architectural trade-offs is essential for selecting the right model for your latency, accuracy, and deployment constraints.',
          ],
        },
        {
          heading: 'Core Concepts',
          paragraphs: [
            'Fundamental building blocks of object detection',
            'Object detection introduces several concepts beyond image classification that are essential to understand for building effective systems. Intersection over Union (IoU) measures the overlap between predicted and ground truth boxes, serving as the foundation for both training and evaluation. Anchor boxes are predefined bounding box shapes that the model learns to adjust, though modern anchor-free approaches predict boxes directly from points. Non-Maximum Suppression (NMS) filters duplicate detections by keeping only the highest-confidence box among overlapping predictions. Feature Pyramid Networks enable detection at multiple scales by creating a hierarchy of feature maps, critical for detecting both large and small objects in the same image.',
          ],
        },
      ],
    },
    concepts: {
      title: 'Core Concepts',
      subtitle: 'Core components and patterns',
      columns: 2,
      cards: [
        {
          className: 'concept-0',
          borderColor: '#3B82F6',
          icon: 'üìê',
          title: '',
          description: 'Overlap ratio for box matching',
          examples: [],
        },
        {
          className: 'concept-1',
          borderColor: '#10B981',
          icon: '‚öì',
          title: '',
          description: 'Predefined box templates',
          examples: [],
        },
        {
          className: 'concept-2',
          borderColor: '#8B5CF6',
          icon: 'üéØ',
          title: '',
          description: 'Filter duplicate detections',
          examples: [],
        },
        {
          className: 'concept-3',
          borderColor: '#F59E0B',
          icon: 'üî∫',
          title: '',
          description: 'Multi-scale feature fusion',
          examples: [],
        },
      ],
    },
    hasSvgViz: true,
    algorithms: {
      type: 'table',
      title: 'Model Comparison',
      subtitle: 'Evaluating approaches and tools',
      headers: ['Name', 'Category', 'Best For', 'Complexity', 'Rating'],
      rows: [
        { icon: 'üõ†Ô∏è', name: 'YOLOv8n', tagText: '37.3%', tagClass: 'tag-blue', bestFor: 'Real-time', complexity: 'medium', rating: '160' },
        { icon: 'üõ†Ô∏è', name: 'YOLOv8x', tagText: '53.9%', tagClass: 'tag-green', bestFor: 'Balanced', complexity: 'medium', rating: '40' },
        { icon: 'üõ†Ô∏è', name: 'RT-DETR-L', tagText: '53.0%', tagClass: 'tag-purple', bestFor: 'Fast + Accurate', complexity: 'medium', rating: '114' },
        { icon: 'üõ†Ô∏è', name: 'Faster R-CNN', tagText: '42.0%', tagClass: 'tag-orange', bestFor: 'Research', complexity: 'medium', rating: '15' },
        { icon: 'üõ†Ô∏è', name: 'DINO-5scale', tagText: '63.2%', tagClass: 'tag-pink', bestFor: 'Max Accuracy', complexity: 'medium', rating: '3' },
        { icon: 'üõ†Ô∏è', name: 'EfficientDet-D4', tagText: '49.4%', tagClass: 'tag-blue', bestFor: 'Efficient', complexity: 'medium', rating: '20' },
        { icon: 'üõ†Ô∏è', name: 'Ultralytics (YOLOv8)', tagText: '', tagClass: 'tag-green', bestFor: '9.2', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'Roboflow', tagText: '', tagClass: 'tag-purple', bestFor: '8.8', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'Detectron2', tagText: '', tagClass: 'tag-orange', bestFor: '8.0', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'MMDetection', tagText: '', tagClass: 'tag-pink', bestFor: '7.8', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üì¶', name: 'AWS Rekognition', tagText: '', tagClass: 'tag-blue', bestFor: '7.6', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üîç', name: 'Google Cloud Vision', tagText: '', tagClass: 'tag-green', bestFor: '7.8', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üî∑', name: 'Azure Custom Vision', tagText: '', tagClass: 'tag-purple', bestFor: '7.4', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'CVAT', tagText: '', tagClass: 'tag-orange', bestFor: '7.2', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
      ],
    },
    tools: {
      title: 'Tools & Libraries',
      subtitle: 'Essential tools and platforms',
      items: [
        { icon: 'U', name: 'Ultralytics', vendor: '', description: 'YOLOv8, simple API, export', tags: [] },
        { icon: 'D2', name: 'Detectron2', vendor: '', description: 'Modular, R-CNN family', tags: [] },
        { icon: 'MM', name: 'MMDetection', vendor: '', description: 'OpenMMLab, 300+ models', tags: [] },
        { icon: 'ü§ó', name: 'Transformers', vendor: '', description: 'DETR, DINO, easy fine-tune', tags: [] },
        { icon: 'CV', name: 'CVAT', vendor: '', description: 'Open-source labeling', tags: [] },
        { icon: 'LS', name: 'Label Studio', vendor: '', description: 'ML-assisted labeling', tags: [] },
        { icon: 'Rb', name: 'Roboflow', vendor: '', description: 'Dataset management', tags: [] },
        { icon: 'SV', name: 'Supervision', vendor: '', description: 'Detection visualization', tags: [] },
        { icon: 'TR', name: 'TensorRT', vendor: '', description: 'NVIDIA optimization', tags: [] },
        { icon: 'ON', name: 'ONNX Runtime', vendor: '', description: 'Cross-platform deploy', tags: [] },
        { icon: 'FL', name: 'FiftyOne', vendor: '', description: 'Dataset exploration', tags: [] },
        { icon: 'WB', name: 'W&B', vendor: '', description: 'Experiment tracking', tags: [] },
      ],
    },
    bestPractices: {
      title: 'Best Practices',
      subtitle: 'Guidelines and recommendations',
      doItems: [
        'Start with Pretrained Weights ‚Äî COCO pretrained models provide strong initialization. Fine-tuning beats training from scratch even for dissimilar domains.',
        'Invest in Annotation Quality ‚Äî Tight, consistent bounding boxes matter. Use multiple annotators, clear guidelines, and review edge cases systematically.',
        'Use Mosaic Augmentation ‚Äî Combining 4 images teaches context awareness and improves small object detection. Standard in YOLO training.',
        'Match Input Resolution to Object Size ‚Äî Small objects need higher resolution (1280px+). Large objects work fine at 640px. Balance speed vs accuracy.',
        'Handle Class Imbalance ‚Äî Use focal loss, class-balanced sampling, or oversample rare classes. Background overwhelms foreground by default.',
        'Tune NMS Thresholds ‚Äî Default IoU threshold (0.5-0.7) may not be optimal. Tune on validation set for your precision-recall needs.',
        'Validate on Real Distribution ‚Äî Test set should match production conditions. Camera angles, lighting, and object scales all affect performance.',
        'Profile Before Optimizing ‚Äî Measure actual bottlenecks (preprocessing, inference, NMS) before applying TensorRT, quantization, or batching.',
        'Use Test-Time Augmentation ‚Äî Multi-scale inference and horizontal flip averaging add 1-3% mAP when latency allows.',
        'Monitor Per-Class Metrics ‚Äî Aggregate mAP hides class-specific failures. Track AP per class and size bucket to identify weak spots.',
      ],
      dontItems: [
      ],
    },
    agent: {
      avatar: 'ü§ñ',
      name: 'DetectionEngineer',
      role: '',
      description: 'Expert agent for designing detection architectures, optimizing anchor configurations, implementing NMS strategies, handling multi-scale detection, and deploying real-time inference pipelines to edge devices.',
      capabilities: [
        'Architecture selection (YOLO vs R-CNN vs DETR)',
        'Anchor box optimization',
        'NMS strategy tuning',
        'Multi-scale detection setup',
        'Real-time optimization',
        'Edge deployment (TensorRT, ONNX)',
      ],
      codeFilename: 'detection_pipeline.py',
      code: ``,
    },
    relatedPages: [
      { number: '23.1', title: 'Image Classification', description: 'CNN and ViT architectures for single-label classification', slug: 'image-classification' },
      { number: '23.3', title: 'Image Segmentation', description: 'Semantic, instance, and panoptic segmentation with SAM', slug: 'image-segmentation' },
      { number: '23.4', title: 'Video Analytics', description: 'Object tracking, action recognition, temporal modeling', slug: 'video-analytics' },
    ],
    prevPage: { title: '23.1 Image Classification', slug: 'image-classification' },
    nextPage: { title: '23.3 Image Segmentation', slug: 'image-segmentation' },
  },
  {
    slug: 'image-segmentation',
    badge: 'üß© Page 23.3',
    title: 'Image Segmentation',
    description: 'Assign class labels to every pixel in an image. From semantic segmentation for scene understanding to instance segmentation for object delineation and the revolutionary Segment Anything Model (SAM) for zero-shot masking, master pixel-level computer vision.',
    accentColor: '#8B5CF6',
    accentLight: '#A78BFA',
    metrics: [
      { value: '83.5%', label: 'ADE20K mIoU SOTA' },
      { value: '1B', label: 'SAM Masks Trained' },
      { value: '150', label: 'ADE20K Classes' },
      { value: '25K', label: 'Training Images' },
    ],
    overview: {
      title: 'Image Segmentation',
      subtitle: '',
      subsections: [
        {
          heading: 'Segmentation Types',
          paragraphs: [
            'Understanding semantic, instance, and panoptic approaches',
            'Image segmentation encompasses three distinct paradigms that serve different use cases and provide varying levels of detail about scene content. Semantic segmentation assigns a class label to every pixel but doesn\'t distinguish between individual instances of the same class‚Äîall cars share the same label regardless of how many are present. Instance segmentation extends object detection by providing pixel-level masks for each detected object, enabling differentiation between multiple instances of the same class. Panoptic segmentation unifies both approaches, providing instance masks for countable "things" (people, cars, animals) while applying semantic labels to amorphous "stuff" (sky, road, grass). The choice between these paradigms depends on whether your application needs to count and track individual objects or simply understand scene composition.',
          ],
        },
        {
          heading: 'Segmentation Architectures',
          paragraphs: [
            'From encoder-decoder networks to foundation models',
            'Segmentation architectures have evolved from simple encoder-decoder structures to sophisticated multi-scale feature fusion networks and, most recently, foundation models capable of zero-shot generalization. The encoder-decoder paradigm, pioneered by FCN and U-Net, uses skip connections to combine high-level semantic features with low-level spatial details for precise boundary recovery. DeepLab introduced atrous (dilated) convolutions to capture multi-scale context without losing resolution, along with Conditional Random Fields (CRFs) for boundary refinement. Mask R-CNN extended Faster R-CNN with a parallel mask prediction branch, becoming the dominant approach for instance segmentation. The Segment Anything Model (SAM) represents a paradigm shift‚Äîtrained on 11 million images with over 1 billion masks, it can segment any object given points, boxes, or text prompts without task-specific training.',
          ],
        },
        {
          heading: 'Core Concepts',
          paragraphs: [
            'Fundamental building blocks of image segmentation',
            'Understanding segmentation requires familiarity with concepts that differ significantly from classification and detection. Encoder-decoder architectures are fundamental‚Äîthe encoder progressively reduces spatial resolution while increasing semantic richness, and the decoder recovers spatial detail for pixel-precise predictions. Skip connections, introduced by U-Net, concatenate encoder features directly to corresponding decoder layers, preserving fine-grained details crucial for accurate boundaries. Atrous/dilated convolutions expand receptive field without losing resolution by inserting holes between kernel weights. Multi-scale processing through feature pyramids or ASPP modules captures both local details and global context essential for understanding objects at varying sizes within the same scene.',
          ],
        },
      ],
    },
    concepts: {
      title: 'Core Concepts',
      subtitle: 'Core components and patterns',
      columns: 2,
      cards: [
        {
          className: 'concept-0',
          borderColor: '#3B82F6',
          icon: 'üîÑ',
          title: '',
          description: 'Compress then reconstruct',
          examples: [],
        },
        {
          className: 'concept-1',
          borderColor: '#10B981',
          icon: '‚è©',
          title: '',
          description: 'Preserve spatial details',
          examples: [],
        },
        {
          className: 'concept-2',
          borderColor: '#8B5CF6',
          icon: 'üî≥',
          title: '',
          description: 'Dilated receptive field',
          examples: [],
        },
        {
          className: 'concept-3',
          borderColor: '#F59E0B',
          icon: 'üî∫',
          title: '',
          description: 'Multi-scale fusion',
          examples: [],
        },
      ],
    },
    hasSvgViz: true,
    algorithms: {
      type: 'card-grid',
      title: 'Model Comparison',
      subtitle: 'Evaluating approaches and tools',
      cards: [
        { icon: 'üõ†Ô∏è', title: 'DeepLabv3+ (ResNet)', subtitle: 'Semantic', description: 'Scene parsing', tags: ['Semantic'] },
        { icon: 'üõ†Ô∏è', title: 'Mask R-CNN', subtitle: 'Instance', description: 'Accurate masks', tags: ['Instance'] },
        { icon: 'üõ†Ô∏è', title: 'YOLOv8-seg', subtitle: 'Instance', description: 'Real-time', tags: ['Instance'] },
        { icon: 'üõ†Ô∏è', title: 'Mask2Former', subtitle: 'Universal', description: 'Unified tasks', tags: ['Universal'] },
        { icon: 'üõ†Ô∏è', title: 'SAM (ViT-H)', subtitle: 'Interactive', description: 'Zero-shot', tags: ['Interactive'] },
        { icon: 'üõ†Ô∏è', title: 'SegFormer-B5', subtitle: 'Semantic', description: 'Transformer', tags: ['Semantic'] },
        { icon: 'üõ†Ô∏è', title: 'Segment Anything (SAM)', subtitle: '', description: '9.0', tags: [] },
        { icon: 'üõ†Ô∏è', title: 'Detectron2', subtitle: '', description: '8.4', tags: [] },
      ],
    },
    tools: {
      title: 'Tools & Libraries',
      subtitle: 'Essential tools and platforms',
      items: [
        { icon: 'SAM', name: 'Segment Anything', vendor: '', description: 'Zero-shot segmentation', tags: [] },
        { icon: 'D2', name: 'Detectron2', vendor: '', description: 'Instance & panoptic', tags: [] },
        { icon: 'MM', name: 'MMSegmentation', vendor: '', description: 'Semantic models', tags: [] },
        { icon: 'ü§ó', name: 'Transformers', vendor: '', description: 'SegFormer, Mask2Former', tags: [] },
        { icon: 'MN', name: 'MONAI', vendor: '', description: '3D medical imaging', tags: [] },
        { icon: 'LS', name: 'Label Studio', vendor: '', description: 'SAM-powered labeling', tags: [] },
        { icon: 'Rb', name: 'Roboflow', vendor: '', description: 'Dataset management', tags: [] },
        { icon: 'SV', name: 'Supervision', vendor: '', description: 'Mask visualization', tags: [] },
        { icon: 'AL', name: 'Albumentations', vendor: '', description: 'Mask-aware transforms', tags: [] },
        { icon: 'SM', name: 'segmentation_models', vendor: '', description: 'PyTorch encoders', tags: [] },
        { icon: 'FL', name: 'FiftyOne', vendor: '', description: 'Mask exploration', tags: [] },
        { icon: 'CV', name: 'CVAT', vendor: '', description: 'Polygon labeling', tags: [] },
      ],
    },
    bestPractices: {
      title: 'Best Practices',
      subtitle: 'Guidelines and recommendations',
      doItems: [
        'Use SAM for Annotation ‚Äî Leverage SAM\'s zero-shot capabilities to accelerate mask annotation. Human annotators refine SAM outputs rather than drawing from scratch.',
        'Combine Loss Functions ‚Äî Mix cross-entropy with Dice loss for balanced optimization. Focal loss addresses class imbalance at pixel level.',
        'Multi-scale Training ‚Äî Train with random scale augmentation (0.5-2.0√ó). Helps model handle objects at varying sizes in production.',
        'Preserve Resolution ‚Äî Use higher resolution inputs when possible. Segmentation quality drops significantly with aggressive downsampling.',
        'Mask-aware Augmentation ‚Äî Use Albumentations or similar for synchronized image-mask transforms. Geometric transforms must apply to both.',
        'Handle Class Imbalance ‚Äî Weight loss by inverse class frequency. Small objects contribute few pixels‚Äîoversample or use focal loss.',
        'Test-Time Augmentation ‚Äî Average predictions across scales and flips for 2-5% mIoU improvement when latency allows.',
        'Validate on Edges ‚Äî Track boundary F-score alongside mIoU. High mIoU can hide poor boundary quality.',
        'Use Pretrained Encoders ‚Äî ImageNet or COCO pretrained backbones provide strong initialization. Fine-tune encoder at lower learning rate.',
        'Post-process Boundaries ‚Äî CRF or learned refinement modules recover details. Especially important for medical and precision applications.',
      ],
      dontItems: [
      ],
    },
    agent: {
      avatar: 'ü§ñ',
      name: 'SegmentationEngineer',
      role: '',
      description: 'Expert agent for designing segmentation architectures, optimizing encoder-decoder networks, implementing loss functions, handling class imbalance, integrating SAM for annotation, and deploying pixel-level prediction systems.',
      capabilities: [
        'Architecture selection (U-Net, DeepLab, Mask2Former)',
        'Loss function design (Dice, Focal, Boundary)',
        'SAM integration for annotation',
        'Multi-scale feature fusion',
        'Medical imaging optimization',
        'Production deployment & monitoring',
      ],
      codeFilename: 'segmentation_pipeline.py',
      code: ``,
    },
    relatedPages: [
      { number: '23.2', title: 'Object Detection', description: 'Bounding box detection with YOLO, R-CNN, and DETR', slug: 'object-detection' },
      { number: '23.4', title: 'Video Analytics', description: 'Object tracking, action recognition, temporal modeling', slug: 'video-analytics' },
      { number: '23.6', title: 'Generative Vision', description: 'Image generation, inpainting, and style transfer', slug: 'generative-vision' },
    ],
    prevPage: { title: '23.2 Object Detection', slug: 'object-detection' },
    nextPage: { title: '23.4 Video Analytics', slug: 'video-analytics' },
  },
  {
    slug: 'video-analytics',
    badge: 'üé¨ Page 23.4',
    title: 'Video Analytics',
    description: 'Process temporal sequences for object tracking, action recognition, and video understanding. From multi-object tracking with DeepSORT to action recognition with Video Transformers, master the techniques that unlock insights from motion and time.',
    accentColor: '#F59E0B',
    accentLight: '#FBBF24',
    metrics: [
      { value: '86.1%', label: 'Kinetics-400 Top-1' },
      { value: '63.1', label: 'MOT17 HOTA' },
      { value: '30 FPS', label: 'Real-time Target' },
      { value: '400', label: 'Action Classes' },
    ],
    overview: {
      title: 'Video Analytics',
      subtitle: '',
      subsections: [
        {
          heading: 'Video Analytics Pipeline',
          paragraphs: [
            'End-to-end flow from video stream to insights',
            'Video analytics pipelines must efficiently process continuous streams while maintaining temporal coherence and meeting latency requirements. The pipeline typically begins with video decoding and frame sampling, where not every frame needs processing‚Äîadaptive sampling based on motion or keyframes reduces compute. Detection runs on sampled frames to locate objects, then tracking algorithms associate detections across time using appearance features and motion prediction. Action recognition modules analyze temporal windows of frames or track sequences to classify behaviors. Finally, business logic transforms raw outputs into actionable events like alerts, counts, or analytics dashboards that drive real-world decisions.',
          ],
        },
        {
          heading: 'Video Analytics Architectures',
          paragraphs: [
            'From tracking algorithms to temporal transformers',
            'Video analytics architectures span multiple complementary domains: object tracking maintains identity across frames, action recognition classifies temporal patterns, and video understanding models capture long-range dependencies. Multi-object tracking (MOT) combines detection with association algorithms‚ÄîDeepSORT pioneered learning appearance embeddings alongside Kalman filter motion prediction, while recent methods like BoT-SORT and ByteTrack achieve state-of-the-art by better handling low-confidence detections and occlusions. For action recognition, the field has evolved from hand-crafted features through 3D CNNs (I3D, SlowFast) to Video Transformers (ViViT, VideoMAE) that model global spatiotemporal relationships with self-attention. Understanding these architectural choices is essential for building systems that balance accuracy, latency, and computational constraints for your specific video analytics application.',
          ],
        },
        {
          heading: 'Core Concepts',
          paragraphs: [
            'Fundamental building blocks of video analytics',
            'Video analytics introduces temporal concepts that extend beyond single-frame image processing. Optical flow estimates pixel-level motion between consecutive frames, providing dense motion information for tracking and action recognition. Multi-object tracking requires solving the data association problem‚Äîmatching detections across frames using appearance similarity (ReID embeddings) and motion prediction (Kalman filters). Temporal modeling captures patterns over time through 3D convolutions that extend spatial filters to the temporal dimension, recurrent networks that maintain hidden state, or attention mechanisms that relate features across arbitrary time distances. Understanding these concepts is essential for designing systems that effectively leverage the temporal structure inherent in video data.',
          ],
        },
      ],
    },
    concepts: {
      title: 'Core Concepts',
      subtitle: 'Core components and patterns',
      columns: 2,
      cards: [
        {
          className: 'concept-0',
          borderColor: '#3B82F6',
          icon: 'üåä',
          title: '',
          description: 'Dense motion estimation',
          examples: [],
        },
        {
          className: 'concept-1',
          borderColor: '#10B981',
          icon: 'üîó',
          title: '',
          description: 'Match detections to tracks',
          examples: [],
        },
        {
          className: 'concept-2',
          borderColor: '#8B5CF6',
          icon: 'üìà',
          title: '',
          description: 'Motion prediction',
          examples: [],
        },
        {
          className: 'concept-3',
          borderColor: '#F59E0B',
          icon: 'üé≠',
          title: '',
          description: 'Appearance similarity',
          examples: [],
        },
      ],
    },
    hasSvgViz: true,
    algorithms: {
      type: 'table',
      title: 'Model Comparison',
      subtitle: 'Evaluating approaches and tools',
      headers: ['Name', 'Category', 'Best For', 'Complexity', 'Rating'],
      rows: [
        { icon: 'üõ†Ô∏è', name: 'ByteTrack', tagText: 'Tracking', tagClass: 'tag-blue', bestFor: 'Real-time MOT', complexity: 'medium', rating: '63.1 HOTA' },
        { icon: 'üõ†Ô∏è', name: 'BoT-SORT', tagText: 'Tracking', tagClass: 'tag-green', bestFor: 'Accuracy', complexity: 'medium', rating: '65.0 HOTA' },
        { icon: 'üõ†Ô∏è', name: 'SlowFast R101', tagText: 'Action', tagClass: 'tag-purple', bestFor: 'Balanced', complexity: 'medium', rating: '79.8% K400' },
        { icon: 'üõ†Ô∏è', name: 'VideoMAE-L', tagText: 'Action', tagClass: 'tag-orange', bestFor: 'Max accuracy', complexity: 'medium', rating: '86.1% K400' },
        { icon: 'üõ†Ô∏è', name: 'X3D-M', tagText: 'Action', tagClass: 'tag-pink', bestFor: 'Efficient', complexity: 'medium', rating: '76.0% K400' },
        { icon: 'üõ†Ô∏è', name: 'SAM 2 Video', tagText: 'Segmentation', tagClass: 'tag-blue', bestFor: 'Video masks', complexity: 'medium', rating: 'SOTA' },
        { icon: 'üõ†Ô∏è', name: 'NVIDIA DeepStream', tagText: '', tagClass: 'tag-green', bestFor: '8.6', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'Ultralytics + Supervision', tagText: '', tagClass: 'tag-purple', bestFor: '8.8', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'PyTorchVideo', tagText: '', tagClass: 'tag-orange', bestFor: '8.2', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'MMTracking', tagText: '', tagClass: 'tag-pink', bestFor: '8.0', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üì¶', name: 'AWS Kinesis Video', tagText: '', tagClass: 'tag-blue', bestFor: '7.4', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üî∑', name: 'Azure Video Indexer', tagText: '', tagClass: 'tag-green', bestFor: '7.2', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'Nx Witness VMS', tagText: '', tagClass: 'tag-purple', bestFor: '7.6', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'OpenCV + FFmpeg', tagText: '', tagClass: 'tag-orange', bestFor: '7.0', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
      ],
    },
    tools: {
      title: 'Tools & Libraries',
      subtitle: 'Essential tools and platforms',
      items: [
        { icon: 'DS', name: 'DeepStream', vendor: '', description: 'NVIDIA streaming analytics', tags: [] },
        { icon: 'CV', name: 'OpenCV', vendor: '', description: 'Video I/O, preprocessing', tags: [] },
        { icon: 'PV', name: 'PyTorchVideo', vendor: '', description: 'Action recognition', tags: [] },
        { icon: 'SV', name: 'Supervision', vendor: '', description: 'ByteTrack, visualization', tags: [] },
        { icon: 'MM', name: 'MMTracking', vendor: '', description: 'MOT model zoo', tags: [] },
        { icon: 'FF', name: 'FFmpeg', vendor: '', description: 'Transcode, stream', tags: [] },
        { icon: 'GS', name: 'GStreamer', vendor: '', description: 'Media framework', tags: [] },
        { icon: 'TR', name: 'TensorRT', vendor: '', description: 'NVIDIA optimization', tags: [] },
        { icon: 'S2', name: 'SAM 2', vendor: '', description: 'Video masks', tags: [] },
        { icon: 'NX', name: 'Nx Witness', vendor: '', description: 'Video management', tags: [] },
        { icon: 'DC', name: 'Decord', vendor: '', description: 'Fast video loading', tags: [] },
        { icon: 'VL', name: 'vidgear', vendor: '', description: 'Multi-threaded I/O', tags: [] },
      ],
    },
    bestPractices: {
      title: 'Best Practices',
      subtitle: 'Guidelines and recommendations',
      doItems: [
        'Invest in Detection Quality ‚Äî Better detection yields better tracking. A 5% detection improvement often outperforms tracker upgrades. High recall matters more than precision.',
        'Use Adaptive Frame Sampling ‚Äî Skip frames in static scenes, process all frames during motion. Motion-based sampling reduces compute 50-80% without accuracy loss.',
        'Handle Camera Motion ‚Äî Moving cameras break tracking assumptions. Use homography estimation or camera motion compensation to stabilize.',
        'Tune Track Lifecycle Parameters ‚Äî Balance responsiveness vs stability. Too aggressive init causes fragmentation; too conservative misses objects.',
        'Include Low-Confidence Detections ‚Äî ByteTrack showed low-confidence detections improve tracking. Second association pass with 0.1 threshold recovers occluded objects.',
        'Profile End-to-End Latency ‚Äî Measure complete pipeline including decode, preprocess, inference, post-process. Bottlenecks often in unexpected places.',
        'Use Hardware Video Decode ‚Äî GPU decode (NVDEC) is 5-10x faster than CPU. Essential for multi-stream deployments. Decord library recommended.',
        'Batch Across Frames ‚Äî Process multiple frames in single inference batch when latency allows. Improves GPU utilization significantly.',
        'Implement Track Smoothing ‚Äî Interpolate tracks across missing detections. Kalman filter provides smooth trajectories even with detection gaps.',
        'Monitor Track Quality Metrics ‚Äî Track average confidence, track age, and fragmentation rate in production. Early warning for degradation.',
      ],
      dontItems: [
      ],
    },
    agent: {
      avatar: 'ü§ñ',
      name: 'VideoAnalyticsEngineer',
      role: '',
      description: 'Expert agent for designing video processing pipelines, implementing multi-object tracking, configuring action recognition models, optimizing streaming analytics, and deploying real-time inference systems on edge and cloud infrastructure.',
      capabilities: [
        'Multi-object tracking (ByteTrack, BoT-SORT)',
        'Action recognition setup',
        'Streaming pipeline design',
        'Real-time optimization',
        'Tracking metrics analysis',
        'Edge deployment (Jetson, DeepStream)',
      ],
      codeFilename: 'video_tracking.py',
      code: ``,
    },
    relatedPages: [
      { number: '23.2', title: 'Object Detection', description: 'YOLO, R-CNN, and DETR for frame-level detection', slug: 'object-detection' },
      { number: '23.3', title: 'Image Segmentation', description: 'SAM and pixel-level masks for video segmentation', slug: 'image-segmentation' },
      { number: '23.5', title: 'OCR & Documents', description: 'Text extraction from video frames and documents', slug: 'ocr-document' },
    ],
    prevPage: { title: '23.3 Image Segmentation', slug: 'image-segmentation' },
    nextPage: { title: '23.5 OCR & Document AI', slug: 'ocr-document' },
  },
  {
    slug: 'ocr-document',
    badge: 'üìÑ Page 23.5',
    title: 'OCR & Document AI',
    description: 'Extract text, structure, and meaning from documents and images. From traditional OCR engines to transformer-based document understanding models, master the techniques that digitize and intelligently process the world\'s documents.',
    accentColor: '#EC4899',
    accentLight: '#F472B6',
    metrics: [
      { value: '99.5%', label: 'Clean Print Accuracy' },
      { value: '94.8%', label: 'DocVQA ANLS' },
      { value: '100+', label: 'Languages' },
      { value: '50ms', label: 'Per-Page Latency' },
    ],
    overview: {
      title: 'OCR & Document AI',
      subtitle: '',
      subsections: [
        {
          heading: 'Document Processing Pipeline',
          paragraphs: [
            'End-to-end flow from raw documents to structured data',
            'Modern document AI pipelines combine multiple specialized stages to transform unstructured documents into machine-readable structured data. The pipeline typically begins with document ingestion handling diverse formats (PDF, images, scans) and preprocessing including deskewing, denoising, and binarization to optimize for OCR. Text detection locates text regions using models like CRAFT or DBNet, followed by text recognition that converts detected regions to character sequences. Layout analysis identifies document structure‚Äîheaders, paragraphs, tables, figures‚Äîpreserving spatial relationships critical for understanding. Finally, downstream processing applies named entity recognition, key-value extraction, or document classification to produce the structured output your business processes require.',
          ],
        },
        {
          heading: 'Document AI Architectures',
          paragraphs: [
            'From OCR engines to transformer-based understanding',
            'Document AI architectures have evolved through three generations with increasingly sophisticated understanding capabilities. Traditional OCR engines like Tesseract focus purely on character recognition, converting pixel patterns to text without understanding document structure. Second-generation systems added layout analysis using object detection to identify regions (text blocks, tables, figures) and reading order. The current generation leverages transformer architectures pre-trained on massive document corpora‚Äîmodels like LayoutLM, Donut, and DocFormer jointly model text, visual features, and spatial layout to achieve document understanding comparable to human performance. These models can answer questions about documents, extract key-value pairs from unseen form layouts, and classify documents with minimal training data by leveraging their pre-trained knowledge of document structure.',
          ],
        },
        {
          heading: 'Core Concepts',
          paragraphs: [
            'Fundamental building blocks of document AI',
            'Understanding document AI requires familiarity with concepts spanning image processing, sequence modeling, and layout understanding. Text detection locates text regions using techniques like CRAFT (Character Region Awareness) or DBNet (Differentiable Binarization), outputting bounding polygons around text areas. Text recognition then converts detected regions to character sequences using encoder-decoder architectures with attention‚Äîmodern approaches like TrOCR use vision transformers for encoding. Layout analysis extends beyond text to identify document structure: tables, figures, headers, and reading order. The key insight in modern document understanding is jointly modeling text content, visual appearance, and spatial position‚Äîthe same words in different locations (header vs. footer) carry different meanings that purely text-based approaches miss.',
          ],
        },
      ],
    },
    concepts: {
      title: 'Core Concepts',
      subtitle: 'Core components and patterns',
      columns: 2,
      cards: [
        {
          className: 'concept-0',
          borderColor: '#3B82F6',
          icon: 'üîç',
          title: '',
          description: 'Locate text regions',
          examples: [],
        },
        {
          className: 'concept-1',
          borderColor: '#10B981',
          icon: 'üî§',
          title: '',
          description: 'Convert to characters',
          examples: [],
        },
        {
          className: 'concept-2',
          borderColor: '#8B5CF6',
          icon: 'üìê',
          title: '',
          description: 'Identify structure',
          examples: [],
        },
        {
          className: 'concept-3',
          borderColor: '#F59E0B',
          icon: 'üìä',
          title: '',
          description: 'Parse rows/columns',
          examples: [],
        },
      ],
    },
    hasSvgViz: true,
    algorithms: {
      type: 'card-grid',
      title: 'Model Comparison',
      subtitle: 'Evaluating approaches and tools',
      cards: [
        { icon: 'üõ†Ô∏è', title: 'PaddleOCR', subtitle: 'OCR Engine', description: 'Multilingual OCR', tags: ['OCR Engine'] },
        { icon: 'üõ†Ô∏è', title: 'Tesseract 5', subtitle: 'OCR Engine', description: 'Simple extraction', tags: ['OCR Engine'] },
        { icon: 'üì¶', title: 'AWS Textract', subtitle: 'Cloud API', description: 'AWS integration', tags: ['Cloud API'] },
        { icon: 'üîç', title: 'Google Doc AI', subtitle: 'Cloud API', description: 'Custom processors', tags: ['Cloud API'] },
        { icon: 'üõ†Ô∏è', title: 'LayoutLMv3', subtitle: 'Transformer', description: 'Fine-tuning', tags: ['Transformer'] },
        { icon: 'üõ†Ô∏è', title: 'GPT-4V / Claude', subtitle: 'LLM', description: 'Novel docs', tags: ['LLM'] },
        { icon: 'üîç', title: 'Google Document AI', subtitle: '', description: '9.0', tags: [] },
        { icon: 'üì¶', title: 'AWS Textract', subtitle: '', description: '8.4', tags: [] },
      ],
    },
    tools: {
      title: 'Tools & Libraries',
      subtitle: 'Essential tools and platforms',
      items: [
        { icon: 'üêô', name: 'PaddleOCR', vendor: '', description: 'Multilingual SOTA', tags: [] },
        { icon: 'T', name: 'Tesseract', vendor: '', description: 'Classic open-source', tags: [] },
        { icon: 'TX', name: 'AWS Textract', vendor: '', description: 'Forms & tables', tags: [] },
        { icon: 'GD', name: 'Google Doc AI', vendor: '', description: 'Custom processors', tags: [] },
        { icon: 'FR', name: 'Azure Form', vendor: '', description: 'Recognizer', tags: [] },
        { icon: 'ü§ó', name: 'Transformers', vendor: '', description: 'LayoutLM, Donut', tags: [] },
        { icon: 'EO', name: 'EasyOCR', vendor: '', description: 'Simple Python API', tags: [] },
        { icon: 'LP', name: 'LayoutParser', vendor: '', description: 'Document structure', tags: [] },
        { icon: 'PM', name: 'pdf2image', vendor: '', description: 'PDF conversion', tags: [] },
        { icon: 'PP', name: 'PyPDF', vendor: '', description: 'PDF parsing', tags: [] },
        { icon: 'PM', name: 'PyMuPDF', vendor: '', description: 'Fast PDF ops', tags: [] },
        { icon: 'UN', name: 'Unstructured', vendor: '', description: 'Document ETL', tags: [] },
      ],
    },
    bestPractices: {
      title: 'Best Practices',
      subtitle: 'Guidelines and recommendations',
      doItems: [
        'Preprocess Aggressively ‚Äî Deskew, denoise, enhance contrast before OCR. Preprocessing often improves accuracy more than model changes. Use OpenCV or imgaug.',
        'Set Confidence Thresholds ‚Äî Route low-confidence results to human review. Don\'t trust 60% confidence extractions. Set per-field thresholds based on criticality.',
        'Validate Extracted Values ‚Äî Check formats (dates, amounts, IDs) against expected patterns. Regex validation catches many extraction errors before downstream use.',
        'Fine-tune on Your Documents ‚Äî Even 100 labeled examples dramatically improve accuracy on domain-specific formats. LayoutLM fine-tuning is straightforward.',
        'Handle Multiple Document Types ‚Äî Classify documents first, then apply type-specific extraction. Don\'t use one model for invoices and contracts.',
        'Build Human-in-the-Loop ‚Äî Design for exception handling from day one. Human review of edge cases improves both accuracy and creates training data.',
        'Test on Production Documents ‚Äî Benchmark accuracy means nothing on your data. Evaluate on representative samples before deployment.',
        'Version Control Processors ‚Äî Document AI models drift. Version processors, track accuracy over time, and maintain rollback capability.',
        'Handle PDF Variations ‚Äî Native PDFs vs scanned images require different pipelines. Detect PDF type and route appropriately.',
        'Monitor Production Accuracy ‚Äî Sample outputs for human review. Track accuracy metrics over time. Document distributions change.',
      ],
      dontItems: [
      ],
    },
    agent: {
      avatar: 'ü§ñ',
      name: 'DocumentAI',
      role: '',
      description: 'Expert agent for designing document processing pipelines, implementing OCR engines, configuring layout analysis, building extraction workflows, integrating with cloud APIs, and deploying production document understanding systems.',
      capabilities: [
        'OCR engine selection and tuning',
        'Layout analysis and table extraction',
        'Key-value pair extraction',
        'Cloud API integration',
        'Document transformer fine-tuning',
        'Validation and quality assurance',
      ],
      codeFilename: 'document_extraction.py',
      code: ``,
    },
    relatedPages: [
      { number: '23.1', title: 'Image Classification', description: 'Document classification and categorization', slug: 'image-classification' },
      { number: '23.4', title: 'Video Analytics', description: 'Text extraction from video frames', slug: 'video-analytics' },
      { number: '23.6', title: 'Generative Vision', description: 'Document generation and enhancement', slug: 'generative-vision' },
    ],
    prevPage: { title: '23.4 Video Analytics', slug: 'video-analytics' },
    nextPage: { title: '23.6 Generative Vision', slug: 'generative-vision' },
  },
  {
    slug: 'generative-vision',
    badge: 'üé® Page 23.6',
    title: 'Generative Vision',
    description: 'Create, edit, and transform images with AI. From diffusion models like Stable Diffusion and DALL-E to GANs and neural style transfer, master the techniques powering the creative AI revolution.',
    accentColor: '#06B6D4',
    accentLight: '#22D3EE',
    metrics: [
      { value: '1024¬≤', label: 'Max Resolution' },
      { value: '<5s', label: 'Generation Time' },
      { value: 'CLIP', label: 'Text Alignment' },
      { value: '100B+', label: 'Training Images' },
    ],
    overview: {
      title: 'Generative Vision',
      subtitle: '',
      subsections: [
        {
          heading: 'Image Generation Pipeline',
          paragraphs: [
            'From text prompt to final image',
            'Modern image generation pipelines center on diffusion models that learn to reverse a noise-adding process. During training, the model learns to predict and remove noise from progressively noisier versions of images. At inference, generation starts from pure random noise and iteratively denoises over many steps (typically 20-50) guided by a text prompt encoded through CLIP or T5. The text encoder converts prompts to embeddings that condition the denoising process at each step. Cross-attention layers in the U-Net architecture allow the model to attend to relevant parts of the prompt while denoising. Advanced pipelines add control through ControlNet (structural guidance), LoRA (style fine-tuning), and IP-Adapter (image prompting), enabling precise creative control while maintaining generation quality.',
          ],
        },
        {
          heading: 'Generative Architectures',
          paragraphs: [
            'From GANs to diffusion models and beyond',
            'Generative vision architectures have evolved through multiple paradigms, each with distinct strengths. GANs (Generative Adversarial Networks) dominated 2014-2021, using a generator-discriminator competition to produce realistic images, excelling at faces (StyleGAN) but struggling with diversity and training stability. Diffusion models emerged as the dominant paradigm from 2022, learning to reverse a noise-adding process through iterative denoising‚Äîthey achieve superior diversity, stability, and controllability at the cost of slower generation. Autoregressive models like DALL-E (original) generate images token-by-token, while VAEs (Variational Autoencoders) provide smooth latent spaces ideal for interpolation. The current frontier combines diffusion with transformers (DiT) and explores flow-matching for faster generation, while multimodal models integrate generation with understanding for richer capabilities.',
          ],
        },
        {
          heading: 'Core Concepts',
          paragraphs: [
            'Fundamental building blocks of generative vision',
            'Understanding generative vision requires familiarity with concepts spanning noise processes, conditioning mechanisms, and quality control. The diffusion process adds Gaussian noise over many timesteps until the image becomes pure noise; generation reverses this by learning to predict and remove noise. Classifier-free guidance (CFG) amplifies prompt adherence by comparing conditional and unconditional predictions‚Äîhigher CFG yields more prompt-aligned but potentially oversaturated images. CLIP (Contrastive Language-Image Pre-training) enables text-to-image alignment by learning a shared embedding space between images and text. Latent space compression through VAEs makes high-resolution generation tractable by operating on 8x downsampled representations. Negative prompts specify what to avoid, while prompt weighting controls the relative influence of different concepts within a prompt.',
          ],
        },
      ],
    },
    concepts: {
      title: 'Core Concepts',
      subtitle: 'Core components and patterns',
      columns: 2,
      cards: [
        {
          className: 'concept-0',
          borderColor: '#3B82F6',
          icon: 'üåä',
          title: '',
          description: 'Noise ‚Üí Image',
          examples: [],
        },
        {
          className: 'concept-1',
          borderColor: '#10B981',
          icon: 'üìä',
          title: '',
          description: 'Prompt adherence',
          examples: [],
        },
        {
          className: 'concept-2',
          borderColor: '#8B5CF6',
          icon: 'üîó',
          title: '',
          description: 'Text-image alignment',
          examples: [],
        },
        {
          className: 'concept-3',
          borderColor: '#F59E0B',
          icon: 'üì¶',
          title: '',
          description: 'Compressed representation',
          examples: [],
        },
      ],
    },
    hasSvgViz: true,
    algorithms: {
      type: 'table',
      title: 'Model Comparison',
      subtitle: 'Evaluating approaches and tools',
      headers: ['Name', 'Category', 'Best For', 'Complexity', 'Rating'],
      rows: [
        { icon: 'üõ†Ô∏è', name: 'DALL-E 3', tagText: '', tagClass: 'tag-blue', bestFor: 'Text rendering, prompts', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'Midjourney v6', tagText: '', tagClass: 'tag-green', bestFor: 'Artistic, aesthetic', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'FLUX.1', tagText: '', tagClass: 'tag-purple', bestFor: 'Best open model', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'SDXL', tagText: '', tagClass: 'tag-orange', bestFor: 'Ecosystem, control', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'Imagen 3', tagText: '', tagClass: 'tag-pink', bestFor: 'Google Cloud integration', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'Ideogram 2', tagText: '', tagClass: 'tag-blue', bestFor: 'Text in images', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'DALL-E 3', tagText: '', tagClass: 'tag-green', bestFor: '8.4', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'Midjourney', tagText: '', tagClass: 'tag-purple', bestFor: '8.6', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'FLUX.1 + Diffusers', tagText: '', tagClass: 'tag-orange', bestFor: '9.0', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'SDXL + ComfyUI', tagText: '', tagClass: 'tag-pink', bestFor: '8.8', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'Replicate', tagText: '', tagClass: 'tag-blue', bestFor: '8.4', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'Stability AI API', tagText: '', tagClass: 'tag-green', bestFor: '8.0', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'Runway Gen-3', tagText: '', tagClass: 'tag-purple', bestFor: '7.8', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
        { icon: 'üõ†Ô∏è', name: 'Civitai Platform', tagText: '', tagClass: 'tag-orange', bestFor: '8.2', complexity: 'medium', rating: '‚≠ê‚≠ê‚≠ê‚≠ê' },
      ],
    },
    tools: {
      title: 'Tools & Libraries',
      subtitle: 'Essential tools and platforms',
      items: [
        { icon: 'ü§ó', name: 'Diffusers', vendor: '', description: 'Best for: Unified API across all diffusion modelsFree ‚Ä¢ Open Source', tags: [] },
        { icon: 'SD', name: 'Stable Diffusion', vendor: '', description: 'Best for: Maximum flexibility & ecosystemFree ‚Ä¢ Self-host GPU', tags: [] },
        { icon: 'üé®', name: 'ComfyUI', vendor: '', description: 'Best for: Complex multi-step workflowsFree ‚Ä¢ Node-based', tags: [] },
        { icon: 'A1', name: 'A1111 WebUI', vendor: '', description: 'Best for: Feature completeness, extensionsFree ‚Ä¢ Most extensions', tags: [] },
        { icon: 'üîÑ', name: 'Replicate', vendor: '', description: 'Best for: Serverless model deployment~$0.0023/sec ‚Ä¢ Pay-per-use', tags: [] },
        { icon: 'CV', name: 'Civitai', vendor: '', description: 'Best for: 500K+ community models/LoRAsFree browse ‚Ä¢ Gen from $5', tags: [] },
        { icon: 'CN', name: 'ControlNet', vendor: '', description: 'Best for: Pose/edge/depth guided generationFree ‚Ä¢ Essential addon', tags: [] },
        { icon: 'IP', name: 'IP-Adapter', vendor: '', description: 'Best for: Style transfer from reference imagesFree ‚Ä¢ Image prompting', tags: [] },
        { icon: 'LO', name: 'LoRA', vendor: '', description: 'Best for: Character/style consistencyFree ‚Ä¢ 10-100MB models', tags: [] },
        { icon: 'KO', name: 'Kohya', vendor: '', description: 'Best for: Easy LoRA training with GUIFree ‚Ä¢ Requires 8GB+ GPU', tags: [] },
        { icon: 'FL', name: 'FLUX', vendor: '', description: 'Best for: SOTA quality, fast inferenceFree (Dev) ‚Ä¢ Pro license avail', tags: [] },
        { icon: 'RW', name: 'RunwayML', vendor: '', description: 'Best for: Text/image to video generation$15-95/mo ‚Ä¢ 125-unlimited creds', tags: [] },
      ],
    },
    bestPractices: {
      title: 'Best Practices',
      subtitle: 'Guidelines and recommendations',
      doItems: [
        'Master Prompt Engineering ‚Äî Be specific: style, lighting, camera angle, quality descriptors. "Professional photo, soft lighting, shallow depth of field" beats vague prompts.',
        'Use Negative Prompts ‚Äî Explicitly exclude failure modes: "blurry, extra limbs, distorted, low quality, watermark". Model-specific negatives matter.',
        'Generate Multiple Candidates ‚Äî Generate 4-16 images per prompt and select best. Faster than perfecting single generations. Use batch processing.',
        'Lock Seeds for Iteration ‚Äî Fix seed while refining prompts to isolate changes. Enables systematic prompt optimization without random variance.',
        'Fine-tune for Consistency ‚Äî LoRA fine-tuning on 20-50 images enables product/character consistency. Essential for brand applications.',
        'Use ControlNet for Structure ‚Äî Guide composition with edges, poses, or depth maps. Critical for consistent product shots and character poses.',
        'Optimize CFG Scale ‚Äî Balance prompt adherence vs image quality. Too low: ignores prompt. Too high: oversaturated, artifacts. Test 5-12 range.',
        'Match Resolution to Model ‚Äî Use native training resolution (512/1024) for best quality. Non-native resolutions produce artifacts. Use upscaling post-generation.',
        'Implement Human Review ‚Äî Never auto-publish generated content. Review for anatomical errors, inappropriate content, brand alignment before release.',
        'Version Control Prompts ‚Äî Track prompts, seeds, and parameters that work. Build prompt libraries. Document model versions used.',
      ],
      dontItems: [
      ],
    },
    agent: {
      avatar: 'ü§ñ',
      name: 'GenerativeVisionEngineer',
      role: '',
      description: 'Expert agent for designing image generation pipelines, crafting effective prompts, implementing ControlNet workflows, fine-tuning models with LoRA, and building production-ready generative applications with appropriate safety measures.',
      capabilities: [
        'Prompt engineering and optimization',
        'ControlNet and IP-Adapter workflows',
        'LoRA fine-tuning and training',
        'Inpainting and image editing',
        'Pipeline optimization',
        'Safety classifier integration',
      ],
      codeFilename: 'generate_with_controlnet.py',
      code: ``,
    },
    relatedPages: [
      { number: '23.3', title: 'Image Segmentation', description: 'SAM for mask-guided generation', slug: 'image-segmentation' },
      { number: '23.4', title: 'Video Analytics', description: 'Video generation and analysis', slug: 'video-analytics' },
      { number: '23.5', title: 'OCR & Documents', description: 'Document generation and enhancement', slug: 'ocr-document' },
    ],
    prevPage: { title: '23.5 OCR & Document AI', slug: 'ocr-document' },
    nextPage: undefined,
  },
]

registerPages('computer-vision', pages)
export default pages
